{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import time\n",
    "import global_config as global_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_page(main_url,page_url,nav_tag,nav_tag_class):\n",
    "    url_s = str(page_url)\n",
    "    r=requests.get(url_s)\n",
    "    s = BeautifulSoup(r.text, 'lxml')\n",
    "    l=[]\n",
    "    #Added bellow 3 lines to work pages which doesn't have navigation\n",
    "    if(nav_tag =='NA' or nav_tag == 'na' or nav_tag_class == 'na' or nav_tag_class == 'na'):\n",
    "        l.append(page_url)\n",
    "        return list(l)\n",
    "    for paragraph in s.find_all(str(nav_tag), class_=str(nav_tag_class)):\n",
    "        #print(paragraph)\n",
    "        for a in paragraph(\"a\"):\n",
    "            if \"http\" in a['href']:\n",
    "                l.append(a['href'])\n",
    "            if \"http\" not in a['href'] and a['href']:\n",
    "                l.append(main_url+a['href'])\n",
    "    return list(set(l))\n",
    "\n",
    "def getproductlink(main_url,page_url,tag,tag_class,sub_tag,sub_tag_class,rt_tag,rt_class,rc_tag,rc_class):\n",
    "    try:\n",
    "        data=requests.get(page_url)\n",
    "        #print(\"Page Title is : %s\" %driver.title)\n",
    "    except Exception as error:\n",
    "        print('error: %s' %error)\n",
    "        print(\"Couldn't access url at indivisual_product_links.py\")\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(data.text, 'lxml')\n",
    "        time.sleep(2)\n",
    "    except Exception as error:\n",
    "        print(\"Couldn't access page source at indivisual_product_links.py\")\n",
    "        print(error)\n",
    "    try:\n",
    "        tag_l = soup.find_all(str(tag),{\"class\":str(tag_class)})\n",
    "        st = '\\n\\n'.join(str(s) for s in tag_l)\n",
    "        lin = BeautifulSoup(str(st),'html.parser')\n",
    "        if tag_l:\n",
    "            link = lin.find_all(str(sub_tag),{\"class\":str(sub_tag_class)})\n",
    "    except:\n",
    "        print(\"Tag/link not found. Probable tag/class-mismatch error.\")\n",
    "    try:\n",
    "        l = []\n",
    "        if link:\n",
    "            for i in link:\n",
    "                '''atag = i.find_all('a')\n",
    "                if \"http\" in atag[0]['href']:\n",
    "                    l.append(atag[0]['href'])\n",
    "                if \"http\" not in atag[0]['href'] and atag[0]['href']:\n",
    "                    l.append(main_url+atag[0]['href'])'''\n",
    "                review = []\n",
    "                revt = i.findall(str(rt_tag),{\"class\":str(rt_class)})\n",
    "                if revt:\n",
    "                    if revt[0].find('p'):\n",
    "                        review.append(revt[0].find('p').text.strip())\n",
    "                    else:\n",
    "                        review.append(revt[0].text.strip())\n",
    "                \n",
    "    except:\n",
    "        print(\"Cound not find the Sub_tag/link\")\n",
    "    s=set(l)\n",
    "    j=0\n",
    "    if s:\n",
    "        for i in s:\n",
    "            #print(str(j)+\" : \"+i)\n",
    "            j += 1\n",
    "    return s\n",
    "\n",
    "def getproductlinks(main_url,page_url,tag,tag_class,sub_tag,sub_tag_class,rt_tag,rt_class,rc_tag,rc_class,nav_tag,nav_tag_class):\n",
    "    all_links=[]\n",
    "    prev_list=[main_url]\n",
    "    latest_links=navigate_page(main_url,page_url,nav_tag,nav_tag_class)\n",
    "    #print(latest_links)\n",
    "\n",
    "    while prev_list[-1]!=latest_links[-1]:\n",
    "        print(latest_links)\n",
    "        prev_list=latest_links\n",
    "        all_links.extend(latest_links)\n",
    "        latest_links=navigate_page(main_url,prev_list[-1],nav_tag,nav_tag_classrt_tag,rt_class,rc_tag,rc_class)\n",
    "\n",
    "    print(all_links)\n",
    "    print(len(all_links))\n",
    "\n",
    "    product_links=[]\n",
    "    for link in all_links:\n",
    "        product_links.extend(getproductlink(main_url,link,tag,tag_class,sub_tag,sub_tag_class))\n",
    "    return product_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
